{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries\n",
    "The absolute first thing we must do is to import all the necessary libraries for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "device = (\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.has_mps else \"cpu\"))\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "After importing everything we need, we have to read the data from the files. We also have to pre-process the data slightly, by e.g. resizing the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 15557\n",
      "Number of test examples: 4002\n"
     ]
    }
   ],
   "source": [
    "# Reading the data from the folders as well as creating dataloaders\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(224, 224))])\n",
    "training_data = torchvision.datasets.ImageFolder(\"data/train/\", transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(\"data/test/\", transform=transform)\n",
    "\n",
    "batch_size = 256\n",
    "training_loader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "print(f\"Number of training examples: {len(training_data)}\")\n",
    "print(f\"Number of test examples: {len(test_data)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing some of the data\n",
    "This code visualizes some of the data in a grid, so we know what we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['Urticaria Hives', 'Seborrheic Keratoses and other Benign Tumors', 'Poison Ivy Photos and other Contact Dermatitis', 'Acne and Rosacea Photos', '.DS_Store', 'Vascular Tumors', 'Eczema Photos', 'Psoriasis pictures Lichen Planus and related diseases', 'Exanthems and Drug Eruptions', 'Lupus and other Connective Tissue diseases', 'Scabies Lyme Disease and other Infestations and Bites', 'Bullous Disease Photos', 'Nail Fungus and other Nail Disease', 'Tinea Ringworm Candidiasis and other Fungal Infections', 'Systemic Disease', 'Light Diseases and Disorders of Pigmentation', 'Atopic Dermatitis Photos', 'Warts Molluscum and other Viral Infections', 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions', 'Melanoma Skin Cancer Nevi and Moles', 'Vasculitis Photos', 'Cellulitis Impetigo and other Bacterial Infections', 'Hair Loss Photos Alopecia and other Hair Diseases', 'Herpes HPV and other STDs Photos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(\"./data/train\")\n",
    "print(f\"classes: {classes}\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a CNN classifier\n",
    "The following code creates a CNN that is used in order to classify the images. Then, we are going to train this classifier, and finally test the classifier on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = (224, 224)\n",
    "        self.num_channels = 16\n",
    "        self.num_labels = 23\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            (self.input_size[0] // 4) * (self.input_size[1] // 4) * self.num_channels,\n",
    "            120,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, self.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 168.785, time: 77.57s\n",
      "Epoch: 2, loss: 163.367, time: 163.22s\n",
      "Epoch: 3, loss: 158.658, time: 241.75s\n",
      "Epoch: 4, loss: 154.696, time: 322.24s\n",
      "Epoch: 5, loss: 151.497, time: 400.47s\n",
      "Epoch: 6, loss: 147.543, time: 477.48s\n",
      "Epoch: 7, loss: 142.911, time: 560.26s\n",
      "Epoch: 8, loss: 138.153, time: 640.93s\n",
      "Epoch: 9, loss: 133.800, time: 717.70s\n",
      "Epoch: 10, loss: 128.108, time: 796.75s\n",
      "Epoch: 11, loss: 121.238, time: 873.80s\n",
      "Epoch: 12, loss: 114.030, time: 954.52s\n",
      "Epoch: 13, loss: 104.592, time: 1035.57s\n",
      "Epoch: 14, loss: 94.438, time: 1112.20s\n",
      "Epoch: 15, loss: 82.399, time: 1192.06s\n",
      "Epoch: 16, loss: 70.600, time: 1270.83s\n",
      "Epoch: 17, loss: 58.559, time: 1349.72s\n",
      "Epoch: 18, loss: 48.428, time: 1427.15s\n",
      "Epoch: 19, loss: 38.200, time: 1506.24s\n",
      "Epoch: 20, loss: 29.778, time: 1582.73s\n",
      "Epoch: 21, loss: 23.914, time: 1659.30s\n",
      "Epoch: 22, loss: 19.371, time: 1736.85s\n",
      "Epoch: 23, loss: 15.538, time: 1813.68s\n",
      "Epoch: 24, loss: 12.484, time: 1890.22s\n",
      "Epoch: 25, loss: 10.800, time: 1969.53s\n",
      "Epoch: 26, loss: 9.707, time: 2048.33s\n",
      "Epoch: 27, loss: 8.374, time: 2133.01s\n",
      "Epoch: 28, loss: 7.791, time: 2215.88s\n",
      "Epoch: 29, loss: 6.997, time: 2299.56s\n",
      "Epoch: 30, loss: 6.510, time: 2383.82s\n",
      "Epoch: 31, loss: 6.057, time: 2467.05s\n",
      "Epoch: 32, loss: 5.799, time: 2551.32s\n",
      "Epoch: 33, loss: 6.053, time: 2628.40s\n",
      "Epoch: 34, loss: 5.751, time: 2705.36s\n",
      "Epoch: 35, loss: 5.462, time: 2781.64s\n",
      "Epoch: 36, loss: 5.270, time: 2856.71s\n",
      "Epoch: 37, loss: 5.158, time: 2931.69s\n",
      "Epoch: 38, loss: 5.104, time: 3006.94s\n",
      "Epoch: 39, loss: 4.756, time: 3082.81s\n",
      "Epoch: 40, loss: 4.669, time: 3157.13s\n",
      "Epoch: 41, loss: 5.599, time: 3231.26s\n",
      "Epoch: 42, loss: 5.399, time: 3305.09s\n",
      "Epoch: 43, loss: 4.968, time: 3378.37s\n",
      "Epoch: 44, loss: 4.449, time: 3453.26s\n",
      "Epoch: 45, loss: 4.522, time: 3530.03s\n",
      "Epoch: 46, loss: 4.374, time: 3606.67s\n",
      "Epoch: 47, loss: 4.549, time: 3683.20s\n",
      "Epoch: 48, loss: 5.140, time: 3759.92s\n",
      "Epoch: 49, loss: 4.804, time: 3836.89s\n",
      "Epoch: 50, loss: 4.883, time: 3915.31s\n",
      "Finished Training, took 3915.31 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "start = time()\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Gathering statistics \n",
    "    end = time()\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {epoch_loss:.3f}, time: {end - start:.2f}s\")\n",
    "\n",
    "end = time()\n",
    "total_time = end - start\n",
    "print(f'Finished Training, took {total_time:.2f} seconds.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 29 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
